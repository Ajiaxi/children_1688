1. Hive 和 Hdfs关系 
- Hive 是一个建立在hadoop文件系统上的数据仓库架构，可以用其对hdfs上数据进行分析与管理。
- 实际上是将hdfs上的文件映射成table（按文件格式创建table,然后hive的数据仓库会生成对应的目录，默认的仓库路径：user/hive/warehouse/tablename，目录名与这个表名相同，这时只要将符合table定义的文件加载到该目录便可通过Hql对整个目录的文件进行查询了。
- 将数据加载到该目录可以用hdfs dfs -put 命令直接添加到该目录；


2. 爬取数据到put到103服务器上的步骤

- 爬取数据
- 然后在103服务器上操作
	- 首先在hdfs上mkdir目录
		- hdfs dfs -ls /user/piday/ #发现没有自己的文件 创建一个
		- hdfs dfs -mkdir -p /user/piday/chenhang/data/childrenwear #childrenwear放童装数据
- 将数据以及 本地写好的sql(sql中的表是和目录名一致的) scp到103服务器 
	- scp 本地文件名 piday@192.168.1.103:data/chenhang/data/childrenwear
- 然后继续在103服务器上操作
	- 然后将数据put到hdfs中
		- hdfs dfs -put 文件名 /user/piday/chenhang/data/childrenwear #
		-- 注意:目录名应和表名相同
- 执行脚本
	- hive -f test.sql

3. hdfs dfs 命令
- 查询命令
hdfs dfs -ls /
hdfs dfs -ls -R /  #ls所有文件

- 新建文件
hdfs dfs -mkdir -p /user/piday/chenhang/data

- 查看内容
hdfs dfs -cat /user/piday/chenhang/data/Kindle2.csv

- 删除文件
hdfs dfs -rm -f /user/piday/chenhang/data/Kindle2.csv

- 删除文件
hdfs dfs -rm -r /user/piday/chenhang/data

- put get
hdfs dfs -put 文件名 /user/piday/chenhang/data/Kindle2
hdfs dfs -get /user/piday/chenhang/data/Kindle2.csv

4. 



	 
